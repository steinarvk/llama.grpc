syntax = "proto3";

package llamagrpc;

service LlamaService {
  rpc Tokenize (TokenizeRequest) returns (TokenizeResponse) {}
  rpc GetVocabulary (GetVocabularyRequest) returns (GetVocabularyResponse) {}

  rpc DoLoadModel (DoLoadModelRequest) returns (DoLoadModelResponse) {}

  rpc DoAddTokensAndCompute (DoAddTokensAndComputeRequest) returns (DoAddTokensAndComputeResponse) {}
  rpc DoPredict (DoPredictRequest) returns (DoPredictResponse) {}

  rpc DoSaveCheckpoint (DoSaveCheckpointRequest) returns (DoSaveCheckpointResponse) {}
  rpc DoRestoreCheckpoint (DoRestoreCheckpointRequest) returns (DoRestoreCheckpointResponse) {}
}

message SessionInfo {
    string session_id = 1;
    string model_name = 2;
    uint64 start_time_unix_nanos = 3;
    ModelInfo model_info = 4;
}

message ModelInfo {
    string model_name = 1;
}

message TokenizeRequest {
    string text = 1;
}

message Token {
    uint32 token_id = 1;
    bytes token_str = 2;
}

message InputTokenSequence {
    repeated uint32 token_id = 1;
}

message TokenLogit {
    Token token = 1;
    double logit = 2;
}

message InputTokens {
    oneof kind {
        string str = 1;
        InputTokenSequence token_ids = 2;
    }
}

message TokenizeResponse {
    repeated Token token = 1;
}

message GetVocabularyRequest {
}

message GetVocabularyResponse {
    repeated Token token = 1;
}

message DoLoadModelRequest {
    string model_name = 1;
}

message DoLoadModelResponse {
    bool model_ready = 1;
    SessionInfo session_info = 2;
}

message DoSaveCheckpointRequest {
    string session_id = 1;
}

message DoSaveCheckpointResponse {
    SessionInfo session_info = 1;
}

message DoRestoreCheckpointRequest {
    string session_id = 1;
}

message DoRestoreCheckpointResponse {
    SessionInfo session_info = 1;
}

message DoAddTokensAndComputeRequest {
    string session_id = 4;

    InputTokens input_tokens = 1;
    bool clear_context_first = 3;

    uint32 top_n_logits = 2;
}

message DoAddTokensAndComputeResponse {
    repeated TokenLogit logit = 1;
    uint32 context_size_tokens = 2;
    uint32 remaining_context_size_tokens = 3;

    SessionInfo session_info = 4;
}

message SessionRestoreInfo {
    string session_id = 1;
    string model_name = 2;
}

message TokenSet {
    repeated uint32 token_id = 1;
}

message LogitFilter {
    oneof kind {
        TokenSet include_only_tokens = 1;
        TokenSet exclude_tokens = 2;
    }
}

message LlamaRepetitionPenalty {
    float intensity = 1;
}

message LogitProcessing {
    uint32 top_n = 1;

    repeated LogitFilter token_filter = 2;

    LlamaRepetitionPenalty llama_repetition_penalty = 3;
}

message SessionHint {
    string session_id = 1;
}

message DoPredictRequest {
    // The model may be loaded on the fly, or rewinded.
    // Or a saved state may be loaded if one exists.

    ModelInfo model_info = 1;
    SessionHint session_hint = 2;

    InputTokens full_context = 3;

    LogitProcessing logit_processing = 4;
}

message DoPredictResponse {
    SessionInfo session_info = 1;

    InputTokenSequence full_input_context = 2;

    repeated TokenLogit next_token_logit = 3;
}