syntax = "proto3";

package llamagrpc;

service LlamaService {
  rpc Tokenize (TokenizeRequest) returns (TokenizeResponse) {}
  rpc GetVocabulary (GetVocabularyRequest) returns (GetVocabularyResponse) {}

  rpc DoLoadModel (DoLoadModelRequest) returns (DoLoadModelResponse) {}

  rpc DoAddTokensAndCompute (DoAddTokensAndComputeRequest) returns (DoAddTokensAndComputeResponse) {}

  rpc DoSaveCheckpoint (DoSaveCheckpointRequest) returns (DoSaveCheckpointResponse) {}
  rpc DoRestoreCheckpoint (DoRestoreCheckpointRequest) returns (DoRestoreCheckpointResponse) {}
}

message SessionInfo {
    string session_id = 1;
    string model_name = 2;
    uint64 start_time_unix_nanos = 3;
}

message TokenizeRequest {
    string text = 1;
}

message Token {
    uint32 token_id = 1;
    bytes token_str = 2;
}

message InputTokenSequence {
    repeated uint32 token_id = 1;
}

message TokenLogit {
    Token token = 1;
    double logit = 2;
}

message InputTokens {
    oneof kind {
        string str = 1;
        InputTokenSequence token_ids = 2;
    }
}

message TokenizeResponse {
    repeated Token token = 1;
}

message GetVocabularyRequest {
}

message GetVocabularyResponse {
    repeated Token token = 1;
}

message DoLoadModelRequest {
    string model_name = 1;
}

message DoLoadModelResponse {
    bool model_ready = 1;
    SessionInfo session_info = 2;
}

message DoSaveCheckpointRequest {
    string session_id = 1;
}

message DoSaveCheckpointResponse {
    SessionInfo session_info = 1;
}

message DoRestoreCheckpointRequest {
    string session_id = 1;
}

message DoRestoreCheckpointResponse {
    SessionInfo session_info = 1;
}

message DoAddTokensAndComputeRequest {
    string session_id = 4;

    InputTokens input_tokens = 1;
    bool clear_context_first = 3;

    uint32 top_n_logits = 2;
}

message DoAddTokensAndComputeResponse {
    repeated TokenLogit logit = 1;
    uint32 context_size_tokens = 2;
    uint32 remaining_context_size_tokens = 3;

    SessionInfo session_info = 4;
}